# Developer: Jinglu Han
# mailbox: admin@de-manufacturing.cn

import re
import json
import logging
import logging
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from fastapi import UploadFile
from app.llm.llm import get_llm
from app.tools.pdf import process_pdf_with_mineru
from app.tools.word import extract_word_content_from_url, extract_word_content_from_bytes
from app.tools.exlsx import download_and_parse_xlsx, extract_excel_content_from_bytes
from app.tools.split_tools import semantic_text_splitter
from app.llm.llm import llm_qwen_vl_max_ainvoke, llm_ollama_vision_ainvoke
import asyncio
import os
import json
from urllib.parse import urlparse
import httpx
from ..core.config import settings # Global import

logger = logging.getLogger(__name__)

DOCUMENT_TYPE_QWEN_VL_DEAL_IMAGE = settings.DOCUMENT_TYPE_QWEN_VL_DEAL_IMAGE

class SummarizeQuestionContentType(BaseModel):
    summarize: str = Field(description="根据问题从文本中提取的内容")

async def process_image(doc_info,question):
    url = doc_info['url']
    file_extension = os.path.splitext(urlparse(url).path)[-1].lower()
    try:
        if file_extension in DOCUMENT_TYPE_QWEN_VL_DEAL_IMAGE:
            content = await llm_qwen_vl_max_ainvoke(question,url)
            return content
        return None
    except Exception as e:
        return f"处理图片时出错: {str(e)}"

async def process_document(doc_info):
    """处理单个文档并返回其内容"""  
    url = doc_info['url']
    file_extension = os.path.splitext(urlparse(url).path)[-1].lower()
    try:
        if file_extension in ['.xlsx', '.xls']:
            content = await download_and_parse_xlsx(url)
        elif file_extension in ['.pdf']:
            # To call the new function, we must first download the content from the URL.
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                response.raise_for_status()  # Raise an exception for bad status codes
                file_bytes = await response.aread()
            
            filename = os.path.basename(urlparse(url).path)
            mineru_result = await process_pdf_with_mineru(file_bytes, filename)
            
            # The result from mineru is a dictionary, e.g., {"result": [...]}.
            # We need to decide how to represent this as 'content'.
            # For now, let's serialize the whole result to JSON string.
            if mineru_result:
                content = json.dumps(mineru_result, ensure_ascii=False)
            else:
                content = ""

        elif file_extension in ['.doc', '.docx']:
            content = await extract_word_content_from_url(url)
        else:
            return None

        return content
    except Exception as e:
        return f"处理文档时出错: {str(e)}"


async def process_images(docu_list, question, max_concurrency=5):
    """并行处理多个图片并返回分割后的内容列表

    Args:
        docu_list: 文档列表，每个文档包含url等信息
        question: 用户问题
        max_concurrency: 最大并发数量

    Returns:
        图片处理结果列表
    """
    # 创建信号量控制并发
    semaphore = asyncio.Semaphore(max_concurrency)

    async def process_image_with_semaphore(doc):
        async with semaphore:
            return await process_image(doc, question)

    # 创建任务列表
    tasks = [process_image_with_semaphore(doc) for doc in docu_list]

    # 并行执行所有任务
    results = await asyncio.gather(*tasks)
    # 合并所有文档内容
    all_contents = []
    for content in results:
        if isinstance(content, str) and content:
            all_contents.append(content)

    return all_contents

async def process_documents(docu_list):
    """并行处理多个文档并返回分割后的内容列表"""
    # 创建任务列表
    tasks = [process_document(doc) for doc in docu_list]

    # 并行执行所有任务
    results = await asyncio.gather(*tasks)

    # 合并所有文档内容
    all_contents = []
    for content in results:
        if isinstance(content, str) and content:
            # If content is a JSON string from a PDF, we need to handle it differently.
            # This part of the logic might need further review based on how this function is used.
            # For now, we attempt to split it as text.
            chunks = await semantic_text_splitter(content)
            if chunks is not None:
                all_contents.append(chunks)

    return all_contents


async def summary_documents_content(docu_str: str, question: str):
    parser = JsonOutputParser(pydantic_object=SummarizeQuestionContentType)
    
    prompt = PromptTemplate(
        template="""
        # 任务说明
            您是一个专业的文档分析助手。您的任务是分析给定的文本内容，判断其是否与用户提出的问题相关，并提取有用信息。

        # 输入内容
            文本内容: {messages}
            用户问题: {question}

        # 分析要求
            1. 仔细阅读文本内容和用户问题
            2. 判断文本内容是否包含与用户问题相关的信息
            3. 如果文本内容与问题相关，请提取出对回答问题有帮助的关键信息并进行简洁总结
            4. 如果文本内容与问题无关，请返回空字符串("")

        # 输出格式
            - 相关时：提供简洁的总结，包含问题相关的关键信息
            - 不相关时：返回空字符串

        {format_instructions}
            """,
        input_variables=["messages", "question"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    
    llm_instance = get_llm(show_think_process=False)
    
    # The chain now returns raw text output from the LLM
    chain = prompt | llm_instance
    
    # Invoke the chain to get the raw output
    raw_result = await chain.ainvoke(
        {"messages": ["user", str(docu_str)], "question": str(question)},
        config={"run_name": f"summarize_question_content_{str(question)}"}
    )
    
    # Manually clean and parse the output
    raw_text_output = raw_result.content if hasattr(raw_result, 'content') else str(raw_result)
    
    # Find the JSON block using regex
    match = re.search(r'\{.*\}', raw_text_output, re.DOTALL)
    
    if match:
        json_str = match.group(0)
        try:
            # Parse the extracted JSON string
            parsed_json = json.loads(json_str)
            return parsed_json.get("summarize")
        except json.JSONDecodeError:
            # Handle cases where the extracted string is not valid JSON
            # This is a fallback, but ideally the regex is good enough
            pass # Or log an error
    
    # If regex fails or parsing fails, try parsing the whole output as a last resort
    try:
        return parser.parse(raw_text_output).get("summarize")
    except Exception:
        # If everything fails, return None or an empty string
        return None


async def process_and_summarize_documents(doc_list, question, max_concurrency=5, max_length=2000):
    """
    处理文档列表并生成总结，控制并发数和总结长度

    Args:
        doc_list: 文档列表，每个文档包含url等信息
        question: 用户问题
        max_concurrency: 最大并发数量
        max_length: 最大字符串长度

    Returns:
        总结列表
    """
    # 处理文档并获取分割后的内容
    chunks = await process_documents(doc_list)
    if not chunks:
        return []

    # 创建信号量控制并发
    semaphore = asyncio.Semaphore(max_concurrency)

    async def summarize_with_semaphore(chunk):
        async with semaphore:
            return await summary_documents_content(chunk, question)

    # 并发调用summary_documents_content
    tasks = [summarize_with_semaphore(chunk) for chunk in chunks]
    summaries = await asyncio.gather(*tasks)

    # 过滤掉空的总结
    summaries = [s for s in summaries if s and s.strip()]

    # 如果总结长度超过最大长度，递归总结
    while summaries and sum(len(s) for s in summaries) > max_length:
        # 将当前总结分成较小的批次
        batch_size = max(1, len(summaries) // 2)
        new_summaries = []

        for i in range(0, len(summaries), batch_size):
            batch = summaries[i:i + batch_size]
            combined_text = "\n".join(batch)
            # 对批次进行再总结
            summary = await summary_documents_content(combined_text, question)
            if summary and summary.strip():
                new_summaries.append(summary)

        summaries = new_summaries

    return summaries


async def process_documents_and_images(doc_list, question, max_concurrency=5, max_length=2000):
    """
    同时处理文档和图片，并返回合并后的结果

    Args:
        doc_list: 文档列表，每个文档包含url等信息
        question: 用户问题
        max_concurrency: 最大并发数量
        max_length: 最大字符串长度

    Returns:
        合并后的总结列表
    """
    # 并行处理文档和图片
    doc_task = process_and_summarize_documents(doc_list, question, max_concurrency, max_length)
    image_task = process_images(doc_list, question, max_concurrency)  # 传递max_concurrency参数

    # 同时等待两个任务完成
    doc_results, image_results = await asyncio.gather(doc_task, image_task)

    # 合并结果
    combined_results = []
    combined_results.extend(doc_results)
    combined_results.extend(image_results)

    return "\n".join(combined_results)


def _extract_text_from_mineru_result(result: dict) -> str:
    """
    从 MinerU 的结构化输出中提取并拼接所有纯文本内容。
    假设 result 的结构是 {"result": [{"type": "text", "text": "...", "content": "..."}, ...]}
    """
    if not result or "result" not in result or not isinstance(result["result"], list):
        return ""

    text_parts = []
    for item in result["result"]:
        # --- DEBUG: Print the full structure of each item ---
        logger.info(f"--- MinerU Result Item: {item} ---")
        # --- END DEBUG ---
        # Broaden the extraction to include any block that has a 'content' key.
        # This will capture text, titles, headers, list items, etc.
        if isinstance(item, dict):
            if "text" in item:
                text_parts.append(str(item["text"]))
            elif "content" in item:
                text_parts.append(str(item["content"]))
    
    extracted_text = "\n".join(text_parts)
    logger.info(f"--- EXTRACTED TEXT from MinerU result (Length: {len(extracted_text)}) ---")
    return extracted_text


def _extract_text_with_pymupdf_fallback(file_bytes: bytes) -> str:
    """
    A fallback function to extract raw text using PyMuPDF if MinerU fails.
    """
    try:
        with fitz.open(stream=file_bytes, filetype="pdf") as doc:
            text = "".join(page.get_text() for page in doc)
        logger.info(f"--- PyMuPDF Fallback: Successfully extracted text (Length: {len(text)}) ---")
        return text
    except Exception as e:
        logger.error(f"--- PyMuPDF Fallback failed: {e} ---")
        return ""


async def extract_text_from_file_content(file_content: bytes, filename: str, question: str = None) -> str:
    """
    从文件内容和文件名中嗅探文件类型，使用相应的解析器提取纯文本。
    
    Args:
        file_content: 文件的字节内容。
        filename: 文件名。
        question: 用户的问题，主要用于图片解析。

    Returns:
        提取出的纯文本内容。
    """
    file_extension = os.path.splitext(filename)[-1].lower()

    # 检查是否为支持的图片类型
    # DOCUMENT_TYPE_QWEN_VL_DEAL_IMAGE 是一个从 settings 加载的 JSON 字符串列表
    supported_image_types = DOCUMENT_TYPE_QWEN_VL_DEAL_IMAGE
    if file_extension.strip('.') in supported_image_types:
        if not question:
            return "Cannot analyze image without a question."
        return await llm_ollama_vision_ainvoke(question, file_content)

    if file_extension == '.pdf':
        # 使用 MinerU 处理 PDF
        mineru_result = await process_pdf_with_mineru(file_content, filename)
        
        # --- DEBUG: Log the raw MinerU result to see its structure and type ---
        logger.info("--- RAW MINERU RESULT ---")
        logger.info(f"--- Type: {type(mineru_result)} ---")
        logger.info(mineru_result)
        logger.info("--- END RAW MINERU RESULT ---")
        # --- END DEBUG ---

        mineru_result_dict = None

        if isinstance(mineru_result, dict):
            # Case 1: The result is already a dictionary
            mineru_result_dict = mineru_result
        elif isinstance(mineru_result, str):
            # Case 2: The result is a JSON string
            try:
                mineru_result_dict = json.loads(mineru_result)
            except json.JSONDecodeError:
                # logger.error("Failed to decode MinerU JSON result string.")
                return ""
        else:
            # Case 3: The result is neither a dict nor a string (e.g., None)
            return ""

        # Now, process the dictionary if we have one
        if mineru_result_dict:
            return _extract_text_from_mineru_result(mineru_result_dict)
        else:
            return ""
    
    elif file_extension in ['.doc', '.docx']:
        # 使用新的基于字节的解析器
        return await extract_word_content_from_bytes(file_content)
    elif file_extension in ['.xlsx', '.xls']:
        # 使用新的基于字节的解析器
        return await extract_excel_content_from_bytes(file_content)
    else:
        return f"Unsupported file type for preview: {file_extension}"

async def get_text_from_uploaded_file(file: UploadFile) -> str:
    """
    从上传的文件中嗅探文件类型，使用相应的解析器提取纯文本。
    这是为文件预览设计的核心函数。

    Args:
        file: FastAPI 的 UploadFile 对象.

    Returns:
        提取出的纯文本内容。
    """
    filename = file.filename
    file_content = await file.read()
    return await extract_text_from_file_content(file_content, filename)

# async def main():
#     doc = [
#         {
#             "file_type": "dsad ipsum",
#             "file_name": "Excepteasdasse ipsum",
#             "url": "http://localhost:8080/image/rosti_ai_logo.png"
#         },
#         {
#             "file_type": "Excepteur dolore in esse ipsum",
#             "file_name": "Excepteur dolore in esse ipsum",
#             "url": "http://localhost:9001/api/v1/download-shared-object/aHR0cDovLzEyNy4wLjAuMTo5MDAwL21tLXJhZy1idWNrZXQvcm9zdGkvQ09QMDNfRjAxNi0wMF9Nb3VsZGluZyUyMGZpcnN0JTIwb3IlMjBsYXN0JTIwc2FtcGxlJTIwaW5zcGVjdGlvbiUyMHJlY29yZCVFNiVCMyVBOCVFNSVBMSU5MSVFOSVBNiU5NiVFNiU5QyVBQiVFNiVBMCVCNyVFNiVBMyU4MCVFOSVBQSU4QyVFOCVBRSVCMCVFNSVCRCU5NV9BMC54bHM_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1MWlJQWDJVWFY1U1lTUE01QzhFRCUyRjIwMjUwNTI3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUyN1QwODM5MjRaJlgtQW16LUV4cGlyZXM9NDMxOTgmWC1BbXotU2VjdXJpdHktVG9rZW49ZXlKaGJHY2lPaUpJVXpVeE1pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SmhZMk5sYzNOTFpYa2lPaUpNV2xKUVdESlZXRlkxVTFsVFVFMDFRemhGUkNJc0ltVjRjQ0k2TVRjME9ETTNPRE0wTkN3aWNHRnlaVzUwSWpvaWJXbHVhVzloWkcxcGJpSjkuR1c5UzItazNqMW1TLVVzVzBpS2tBSTNJUWV2STBRY280VzlzMV9pR3Z5dGUxazdhS2F0TUMyNzhjcVkwT1JBS1Z0OFJSY3pBZ2t4Tk9TWm90aG9NV2cmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JnZlcnNpb25JZD1udWxsJlgtQW16LVNpZ25hdHVyZT0wNDFhNzRlMDVlZTAzYjJjNjYxMjBhYWViZmQwNDU0ODBkYzAwYTdjMDBkNzJhZTUyY2Q5MjAwNWU3MGExNzhj"
#         }
#     ]
#     #a = await process_documents_and_images(doc, "经理电话")
#     a = await process_and_summarize_documents(doc, "经理电话")
#     print(a)
#     print(len(a))


# if __name__ == "__main__":
#     asyncio.run(main())
